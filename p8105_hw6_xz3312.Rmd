---
title: "p8105_hw6_xz3312"
author: "Eric Zhang"
date: "2025-12-01"
output: html_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(ggplot2)
library(broom)
library(p8105.datasets)
library(modelr)
data("weather_df")
```

# Problem 1 
The Washington Post has gathered data on homicides in 50 large U.S. cities and made the data available through a GitHub repository

```{r}
# load data from GitHub

homicides_data <- read_csv(
  "https://raw.githubusercontent.com/washingtonpost/data-homicides/master/homicide-data.csv"
)
```
### Part a
Create a city_state variable (e.g. “Baltimore, MD”), and a binary variable indicating whether the homicide is solved. Omit cities Dallas, TX; Phoenix, AZ; and Kansas City, MO – these don’t report victim race. Also omit Tulsa, AL – this is a data entry mistake. For this problem, limit your analysis those for whom victim_race is white or black. Be sure that victim_age is numeric.

```{r}
homicides_clean <- homicides_data %>% 
  mutate(
    city_state  = str_c(city, state, sep = ", "),
    resolved    = if_else(disposition == "Closed by arrest", 1, 0),

    # fix victim_age before numeric conversion
    victim_age  = na_if(victim_age, "Unknown"),
    victim_age  = as.numeric(victim_age),

    # convert sex and race to factors
    victim_sex  = factor(victim_sex),
    victim_race = factor(victim_race),

    # ensure female is the reference level
    victim_sex  = fct_relevel(victim_sex, "Female")
  ) %>% 
  filter(
    !city_state %in% c("Dallas, TX", "Phoenix, AZ", "Kansas City, MO", "Tulsa, AL"),
    victim_race %in% c("White", "Black")
  ) %>% 
  drop_na(resolved, victim_age, victim_sex, victim_race)
```

### Part b
For the city of Baltimore, MD, use the glm function to fit a logistic regression with resolved vs unresolved as the outcome and victim age, sex and race as predictors. Save the output of glm as an R object; apply the broom::tidy to this object; and obtain the estimate and confidence interval of the adjusted odds ratio for solving homicides comparing male victims to female victims keeping all other variables fixed.
```{r}
baltimore <- homicides_clean %>% 
  filter(city_state == "Baltimore, MD")

fit_baltimore <- glm(
  resolved ~ victim_age + victim_sex + victim_race,
  data = baltimore,
  family = binomial()
)

baltimore_or <- fit_baltimore %>% 
  broom::tidy(conf.int = TRUE, exponentiate = TRUE) %>% 
  filter(term == "victim_sexMale") %>% 
  select(term, estimate, conf.low, conf.high)

baltimore_or
```
Using the fitted logistic regression model for Baltimore, MD, the adjusted odds ratio for solving a homicide for male victims compared to female victims, holding age and race constant, is
`r round(baltimore_or$estimate, 2)`,
with a 95% confidence interval (`r round(baltimore_or$conf.low, 2)`, `r round(baltimore_or$conf.high, 2)`).

Since this odds ratio is below 1, the model suggests that homicides involving male victims are less likely to be solved than those involving female victims, after adjusting for age and race.

### Part c
Now run glm for each of the cities in your dataset, and extract the adjusted odds ratio (and CI) for solving homicides comparing male victims to female victims. Do this within a “tidy” pipeline, making use of purrr::map, list columns, and unnest as necessary to create a dataframe with estimated ORs and CIs for each city.
```{r}
city_ors <- homicides_clean %>% 
  nest(data = -city_state) %>% 
  mutate(
    fit = map(
      data,
      ~ glm(
        resolved ~ victim_age + victim_sex + victim_race,
        data   = .x,
        family = binomial()
      )
    ),
    tidy = map(
      fit,
      ~ tidy(.x, conf.int = TRUE, exponentiate = TRUE)
    )
  ) %>% 
  select(city_state, tidy) %>% 
  unnest(tidy) %>% 
  filter(term == "victim_sexMale") %>% 
  select(city_state, estimate, conf.low, conf.high)

city_ors
```
Across cities, the adjusted odds ratios comparing male to female victims vary
substantially. For example, Albuquerque has an OR of `r round(city_ors$estimate[city_ors$city_state == "Albuquerque, NM"], 2)`
with a 95% CI of (`r round(city_ors$conf.low[city_ors$city_state == "Albuquerque, NM"], 2)`,
`r round(city_ors$conf.high[city_ors$city_state == "Albuquerque, NM"], 2)`), indicating higher odds of a homicide being solved
for male victims. In contrast, Baltimore shows a much lower OR of
`r round(city_ors$estimate[city_ors$city_state == "Baltimore, MD"], 2)` (95% CI:
`r round(city_ors$conf.low[city_ors$city_state == "Baltimore, MD"], 2)`,
`r round(city_ors$conf.high[city_ors$city_state == "Baltimore, MD"], 2)`),
suggesting male-victim homicides are less likely to be solved than female-victim homicides.

Many cities, such as Boston (`r round(city_ors$estimate[city_ors$city_state == "Boston, MA"], 2)`), have odds ratios below 1,
while others, such as Atlanta (`r round(city_ors$estimate[city_ors$city_state == "Atlanta, GA"], 2)`), are close to 1,
indicating little difference by victim sex. Overall, most confidence intervals include values below 1,
suggesting that in many cities male-victim homicides tend to be solved at lower rates.

### Part d
Create a plot that shows the estimated ORs and CIs for each city. Organize cities according to estimated OR, and comment on the plot.
```{r}
city_ors %>% 
  ggplot(aes(
    x = reorder(city_state, estimate),
    y = estimate,
    ymin = conf.low,
    ymax = conf.high
  )) +
  geom_pointrange() +
  geom_hline(yintercept = 1, linetype = "dashed") +
  coord_flip() +
  labs(
    x = "City",
    y = "Adjusted OR (Male vs Female)",
    title = "Odds of homicide being solved: male vs female victims"
  )
```
The plot shows substantial variation across cities in the odds of solving homicides for male versus female victims. Most cities have odds ratios below 1, suggesting that homicides involving male victims are generally less likely to be solved. Only a few cities have odds ratios above 1, and many confidence intervals are wide, indicating uncertainty in estimates of several cities.

# Problem 2
For this problem, we’ll use the Central Park weather data.

### Part a
The code chunk below will import these data from the p8105.datasets package.We’ll focus on a simple linear regression with tmax as the response with tmin and prcp as the predictors, and are interested in the distribution of two quantities estimated from these data:

r_hat_square
beta_hat_1 / beta_hat_2

The linear model we will fit would look like: tmax = b0 + b1(tmin) +b2(prcp)

Use 5000 bootstrap samples and, for each bootstrap sample, produce estimates of these two quantities. 
```{r}
set.seed(1)

boot_results <- weather_df %>% 
  modelr::bootstrap(5000) %>% 
  mutate(
    # fit tmax ~ tmin + prcp in each bootstrap sample
    model  = map(strap, ~ lm(tmax ~ tmin + prcp, data = .x)),
    gl     = map(model, broom::glance),
    td     = map(model, broom::tidy),

    # r-hat squared from each model
    r_hat_sq = map_dbl(gl, "r.squared"),

    # β1-hat * β2-hat: coefficients for tmin and prcp
    beta_prod = map_dbl(
      td,
      ~ {
        b1 <- .x$estimate[.x$term == "tmin"]
        b2 <- .x$estimate[.x$term == "prcp"]
        b1 * b2
      }
    )
  ) %>% 
  select(r_hat_sq, beta_prod)

head(boot_results)
```
Across the 5000 bootstrap samples, the estimates of `r^2` remain very stable,
with a mean around `r round(mean(boot_results$r_hat_sq), 3)` and most values
close to `r round(boot_results$r_hat_sq[1], 3)`, indicating that the model
explains a large proportion of the variability in *tmax*. In contrast, the
product `β̂1 β̂2` is consistently small and negative, with typical values near
`r round(boot_results$beta_prod[1], 5)`, suggesting a weak combined effect of
*tmin* and *prcp* on *tmax* across bootstrap samples.

### Part b
Plot the distribution of your estimates, and describe these in words. Using the 5000 bootstrap estimates, identify the 2.5% and 97.5% quantiles to provide a 95% confidence interval for r_hat_square and beta_1_hat / beta_2_hat
```{r}
# Plot 1 distribution of r hat squared
boot_results %>% 
  ggplot(aes(x = r_hat_sq)) +
  geom_histogram(bins = 40, color = "white") +
  labs(
    title = "Bootstrap Distribution of Estimated Model Fit",
    subtitle = expression(paste("Distribution of ", hat(r)^2, " from 5000 bootstrap samples")),
    x = expression(hat(r)^2),
    y = "Count"
  )

# Plot 2 distribution of beta
boot_results %>% 
  ggplot(aes(x = beta_prod)) +
  geom_histogram(bins = 40, color = "white") +
  labs(
    title = "Bootstrap Distribution of Coefficient Product",
    subtitle = expression(paste("Distribution of ", hat(beta)[1] * hat(beta)[2],
                                " from 5000 bootstrap samples")),
    x = expression(hat(beta)[1] * hat(beta)[2]),
    y = "Count"
  )
```
The bootstrap distribution of model fit shows that `r round(mean(boot_results$r_hat_sq), 3)`
is centered near 0.94 with a fairly symmetric shape, indicating that the model
consistently explains a large proportion of variability in *tmax* across bootstrap
samples. In contrast, the distribution of `r paste0("β̂1β̂2")` is centered near
`r round(mean(boot_results$beta_prod), 5)` and remains consistently negative,
suggesting a weak combined effect of `tmin` and `prcp` on `tmax`.

# Problem 3
In this problem, you will analyze data gathered to understand the effects of several variables on a child’s birthweight. This dataset consists of roughly 4000 children and includes the following variables:
babysex: baby’s sex (male = 1, female = 2)
bhead: baby’s head circumference at birth (centimeters)
blength: baby’s length at birth (centimeters)
bwt: baby’s birth weight (grams)
delwt: mother’s weight at delivery (pounds)
fincome: family monthly income (in hundreds, rounded)
frace: father’s race (1 = White, 2 = Black, 3 = Asian, 4 = Puerto Rican, 8 = Other, 9 = Unknown)
gaweeks: gestational age in weeks
malform: presence of malformations that could affect weight (0 = absent, 1 = present)
menarche: mother’s age at menarche (years)
mheigth: mother’s height (inches)
momage: mother’s age at delivery (years)
mrace: mother’s race (1 = White, 2 = Black, 3 = Asian, 4 = Puerto Rican, 8 = Other)
parity: number of live births prior to this pregnancy
pnumlbw: previous number of low birth weight babies
pnumgsa: number of prior small for gestational age babies
ppbmi: mother’s pre-pregnancy BMI
ppwt: mother’s pre-pregnancy weight (pounds)
smoken: average number of cigarettes smoked per day during pregnancy
wtgain: mother’s weight gain during pregnancy (pounds)


### Part a 
Load and clean the data for regression analysis (i.e. use appropriate variable names, convert numeric to factor where appropriate, check for the presence of missing data, etc.).
```{r}
# load the data
birthweight <- read_csv("birthweight.csv") %>% 
  mutate(
    babysex = factor(babysex, labels = c("Male", "Female")),
    frace   = factor(frace),
    mrace   = factor(mrace),
    malform = factor(malform),
  )

summary(birthweight)
```

### Part b
Propose a regression model for birthweight. This model may be based on a hypothesized structure for the factors that underly birthweight, on a data-driven model-building process, or a combination of the two. Describe your modeling process and show a plot of model residuals against fitted values – use add_predictions and add_residuals in making this plot.

For my primary birthweight model, I select the following predictors:

bwt ∼ blength + bhead + gaweeks + ppbmi + momage + wtgain + smoken


These predictors were chosen based on logical and biological reasonsings.

blength (baby length) and bhead (head circumference) are direct physical measurements of the newborn and are known to be strongly associated with birthweight.

gaweeks (gestational age) is one of the strongest predictors of birthweight, since weight increases steadily as the pregnancy progresses.

ppbmi (mother’s pre-pregnancy BMI) may reflect maternal nutritional status prior to gestation.

wtgain (mother’s pregnancy weight gain) captures fetal growth support during pregnancy.

momage (mother’s age) may affect fetal development and pregnancy health.

smoken (maternal smoking) is a negative predictor of fetal growth.
```{r}
mod_main <- lm(
  bwt ~ blength + bhead + gaweeks + ppbmi + momage + wtgain + smoken,
  data = birthweight
)

tidy(mod_main)
glance(mod_main)
```
The model explains a substantial amount of variability in birthweight, with an 
\(R^2\) of `r round(glance(mod_main)$r.squared, 3)` and a residual standard error 
of about `r round(glance(mod_main)$sigma, 1)` grams. Key predictors show strong 
positive associations: each additional centimeter of baby length adds 
`r round(tidy(mod_main)$estimate[tidy(mod_main)$term == "blength"], 1)` grams, 
and each additional centimeter of head circumference adds 
`r round(tidy(mod_main)$estimate[tidy(mod_main)$term == "bhead"], 1)` grams. 
Gestational age also increases birthweight by 
`r round(tidy(mod_main)$estimate[tidy(mod_main)$term == "gaweeks"], 1)` grams per week. 
Maternal BMI and pregnancy weight gain contribute modest increases, while smoking 
is associated with a decrease of 
`r round(tidy(mod_main)$estimate[tidy(mod_main)$term == "smoken"], 1)` grams per cigarette per day.

```{r}
# Residual vs fitted plot
birthweight_main <- birthweight %>% 
  add_predictions(mod_main) %>% 
  add_residuals(mod_main)

birthweight_main %>% 
  ggplot(aes(x = pred, y = resid)) +
  geom_point(alpha = 0.3) +
  geom_hline(yintercept = 0, linetype = "dashed") +
  labs(
    title = "Residuals vs Fitted Values: Main Birthweight Model",
    x = "Fitted birthweight (grams)",
    y = "Residuals"
  )
```
Residuals appear roughly centered around zero across the range of fitted values with no strong patterns, suggesting that linearity and homoscedasticity are reasonably satisfied. A few large residuals are seen, but overall the model fit is good.

### Part c
Compare your model to two others:
A) One using length at birth and gestational age as predictors (main effects only)
B) One using head circumference, length, sex, and all interactions (including the three-way interaction) between these
Make this comparison in terms of the cross-validated prediction error; use crossv_mc and functions in purrr as appropriate.
```{r}
# Two comparison models (required by assignment)**

# *Model A: Uses only baby length and gestational age (main effects)
mod_simple <- lm(
  bwt ~ blength + gaweeks,
  data = birthweight
)

# *Model B: Head circumference, length, baby sex, and all interactions (including 3-way interaction)
mod_inter <- lm(
  bwt ~ bhead * blength * babysex,
  data = birthweight
)

set.seed(1)

cv_df <- crossv_mc(birthweight, n = 100)

cv_results <- cv_df |>
  mutate(
    train = map(train, as_tibble),
    test  = map(test, as_tibble),

    fit_main = map(train, ~ lm(
      bwt ~ blength + bhead + gaweeks + ppbmi + momage + wtgain + smoken,
      data = .x
    )),
    fit_simple = map(train, ~ lm(
      bwt ~ blength + gaweeks,
      data = .x
    )),
    fit_inter = map(train, ~ lm(
      bwt ~ bhead * blength * babysex,
      data = .x
    )),

    rmse_main   = map2_dbl(fit_main,   test, ~ rmse(.x, .y)),
    rmse_simple = map2_dbl(fit_simple, test, ~ rmse(.x, .y)),
    rmse_inter  = map2_dbl(fit_inter,  test, ~ rmse(.x, .y))
  )

cv_summary <- cv_results %>% 
  summarise(
    rmse_main   = mean(rmse_main),
    rmse_simple = mean(rmse_simple),
    rmse_inter  = mean(rmse_inter)
  )

cv_summary
```
Based on the cross-validated RMSE values, the main model performs best with an
average RMSE of `r round(cv_summary$rmse_main, 1)`, lower than both the simpler 
length–gestational age model (`r round(cv_summary$rmse_simple, 1)`) and the 
interaction-heavy model (`r round(cv_summary$rmse_inter, 1)`). This indicates that 
the main model achieves the strongest predictive accuracy without unnecessary 
complexity.

